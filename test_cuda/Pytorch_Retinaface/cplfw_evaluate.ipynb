{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb6c1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/test_cuda/\")\n",
    "import shutil\n",
    "import math\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "import cv2\n",
    "from models.retinaface import RetinaFace\n",
    "from utils.box_utils import decode, decode_landm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import crop, center_crop, rotate, InterpolationMode, pad, resize\n",
    "#from torchvision.transforms.functional import rotate\n",
    "#from torchvision.transforms.functional import InterpolationMode\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization, training, extract_face\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "from torchvision import datasets, transforms\n",
    "from model import Backbone, Arcface, MobileFaceNet, Am_softmax, l2_norm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0106761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    #print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    #print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    #print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "802f2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    #print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c52ed02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "        print(\"Model loaded to GPU\")\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c4a3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_model(network=\"resnet50\"):\n",
    "    if network == \"mobile0.25\":\n",
    "        cfg = cfg_mnet\n",
    "        trained_model = \"./weights/mobilenet0.25_Final.pth\"\n",
    "    elif network == \"resnet50\":\n",
    "        cfg = cfg_re50\n",
    "        trained_model = \"./weights/Resnet50_Final.pth\"\n",
    "    # net and model\n",
    "    net = RetinaFace(cfg=cfg, phase = 'test')\n",
    "    net = load_model(net, trained_model, False)\n",
    "    net.eval()\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\") # Defines the computation device (cuda:0 => GPU)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    return net, cfg, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46740ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_select(dets, selec_thresh, max_score=False):\n",
    "    previous_area = 0\n",
    "    max_area = 0\n",
    "    prev_coords = np.zeros_like(dets[0])\n",
    "    coords = np.zeros_like(dets[0])\n",
    "    \n",
    "    #print(\"face select: \\n\", dets, len(dets))\n",
    "    if max_score == True: #Pre-crop that selects the face with higher score\n",
    "        score = 0\n",
    "        max_score = 0\n",
    "        prev_crop_coords = np.zeros_like(dets[0])\n",
    "        crop_coords = np.zeros_like(dets[0])\n",
    "\n",
    "        for b in dets:\n",
    "            if len(dets) == 1:\n",
    "                crop_coords[:] = b\n",
    "                \n",
    "            if b[4]>score:\n",
    "                score = b[4]\n",
    "                prev_coords[:] = b\n",
    "            else:\n",
    "                max_score = score\n",
    "                crop_coords[:] = prev_coords\n",
    "        \n",
    "        #crop_coords = list(map(int, crop_coords))\n",
    "        #margin = 0.2 #x% around the bounding box\n",
    "        #crop_height = crop_coords[3]-crop_coords[1] #ymax-ymin\n",
    "        #crop_width = crop_coords[2]-crop_coords[0] #xmax-xZmin\n",
    "        #crop_coords = (crop_coords[1]*(1-margin), crop_coords[0]*(1-margin), crop_height*(1+margin), crop_width*(1+margin))\n",
    "        \n",
    "        return crop_coords\n",
    "\n",
    "    else:\n",
    "        for b in dets:\n",
    "            height = b[3]-b[1] #ymax-ymin\n",
    "            width = b[2]-b[0] #xmax-xmin\n",
    "            bbox_area = int(width)*int(height)\n",
    "\n",
    "\n",
    "            #print(b[4])\n",
    "            #if b[4]<0.001:\n",
    "                #for i in range(len(coords)):\n",
    "                    #coords[i] = prev_coords[i]\n",
    "                #continue\n",
    "\n",
    "            #b = list(map(int, b))\n",
    "            for i in range(len(b)): # Turns into int every element from the detections except the score\n",
    "                if i == 4:\n",
    "                    continue\n",
    "                else:\n",
    "                    b[i] = int(b[i])\n",
    "\n",
    "            #print(\"test\", b, bbox_area, previous_area)              \n",
    "\n",
    "            if len(dets) == 1: # Only one face present in the picture\n",
    "                max_area = bbox_area\n",
    "                for i in range(len(coords)):\n",
    "                    coords[i] = b[i]\n",
    "            else:\n",
    "                if bbox_area > previous_area:\n",
    "                    previous_area = bbox_area\n",
    "                    for i in range(len(prev_coords)):\n",
    "                        prev_coords[i] = b[i]\n",
    "                else:\n",
    "                    max_area = previous_area\n",
    "                    for i in range(len(coords)):\n",
    "                        coords[i] = prev_coords[i]\n",
    "\n",
    "\n",
    "        face = np.append(coords, max_area)\n",
    "\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad3ffa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_align(img, dets, selec_thresh, net, cfg, device, final_dir, save=False):\n",
    "    '''\n",
    "    b[0], b[1] is the top left corner of the bounding box\n",
    "    b[2], b[3] is the lower right corner of the bounding box\n",
    "    b[4] relates to the the score of the detection\n",
    "    b[5], b[6] is the left eye\n",
    "    b[7], b[8] is the right eye\n",
    "    b[9], b[10] is the nose\n",
    "    b[11], b[12] is the left of the mouth\n",
    "    b[13], b[14] is the right of the mouth\n",
    "    '''\n",
    "    #print(\"dets 2 - crop align (pre): \\n\", dets)\n",
    "    #img_raw = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    face_coords = face_select(dets, selec_thresh)\n",
    "    face_coords = list(map(int, face_coords)) # Coordinates must be integers\n",
    "    \n",
    "    \n",
    "    # -------------------- Rotation Stage ---------------------\n",
    "    left_eye = (face_coords[5], face_coords[6]) # Components: (x, y)\n",
    "    right_eye = (face_coords[7], face_coords[8])\n",
    "    if left_eye[1] > right_eye[1]:               # Right eye is higher\n",
    "        # Clock-wise rotation\n",
    "        aux_point = (right_eye[0], left_eye[1])\n",
    "        a = right_eye[0] - left_eye[0]\n",
    "        b = right_eye[1] - aux_point[1]\n",
    "        \n",
    "        #cv2.circle(img_raw, left_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, right_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, aux_point, 10, (0, 255, 0), 4)\n",
    "        \n",
    "        #cv2.line(img_raw, left_eye, right_eye, (23, 23, 23), 2)\n",
    "        #cv2.line(img_raw, aux_point, right_eye, (23, 23, 23), 2)\n",
    "        #cv2.line(img_raw, left_eye, aux_point, (23, 23, 23), 2)\n",
    "        #plt.imshow(cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)) \n",
    "        try:\n",
    "            theta = np.rad2deg(np.arctan(b/a)) # Angle of rotation in degrees\n",
    "            rotated_tensor = rotate(img.squeeze(), angle=theta, interpolation=InterpolationMode.BILINEAR, center=right_eye)\n",
    "        except ZeroDivisionError:\n",
    "            rotated_tensor = img.squeeze()\n",
    "            \n",
    "        #print(\"Right eye is higher, therefore, a clock-wise rotation of {} is applied\".format(-theta))\n",
    "        \n",
    "\n",
    "    else:                                        # Left eye is higher\n",
    "        # Counter clock-wise rotation\n",
    "        aux_point = (left_eye[0], right_eye[1])\n",
    "        a = right_eye[0] - left_eye[0]\n",
    "        b = left_eye[1] - aux_point[1]\n",
    "        \n",
    "        #cv2.circle(img_raw, left_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, right_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, aux_point, 10, (0, 255, 0), 4)\n",
    "        \n",
    "        #plt.imshow(img_raw)\n",
    "        try:\n",
    "            theta = np.rad2deg(np.arctan(b/a))\n",
    "            #print(\"Left eye is higher, therefore, a clock-wise rotation of {} degrees is applied\".format(-theta))\n",
    "            rotated_tensor = rotate(img.squeeze(), angle=-theta, interpolation=InterpolationMode.BILINEAR, center=left_eye)\n",
    "        except ZeroDivisionError:\n",
    "            rotated_tensor = img.squeeze()\n",
    "    \n",
    "    #margin = 0.5 #x% around the bounding box\n",
    "    #crop_height = face_coords[3]-face_coords[1] #ymax-ymin\n",
    "    #crop_width = face_coords[2]-face_coords[0] #xmax-xZmin\n",
    "    #crop_coords = (face_coords[1]*(1-margin), face_coords[0]*(1-margin), crop_height*(1+margin), crop_width*(1+margin))\n",
    "    #crop_coords = list(map(int, crop_coords))\n",
    "    #rotated_tensor = crop(rotated_tensor, *crop_coords)\n",
    "    #plt.imshow(rotated_tensor.squeeze().permute(1, 2, 0).cpu().numpy().astype(int))\n",
    "    \n",
    "    # -------------------- New Bounding Box computing ---------------------\n",
    "    # The image is rotated, a new bbox must be generated. \n",
    "    \n",
    "    # TBD: optimization by performing a preliminary crop in order to try and isolate only the relevant face\n",
    "    \n",
    "    loc, conf, _ = net(rotated_tensor.unsqueeze(0))  # Forward pass that gives the results <--------------\n",
    "    \n",
    "    im_height = rotated_tensor.shape[1]\n",
    "    im_width = rotated_tensor.shape[2]\n",
    "    \n",
    "    resize1 = 1\n",
    "    new_scale = torch.Tensor([rotated_tensor.shape[2], rotated_tensor.shape[1], rotated_tensor.shape[2], rotated_tensor.shape[1]])\n",
    "    new_scale = new_scale.to(device)\n",
    "    \n",
    "    new_priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    new_priors = new_priorbox.forward()\n",
    "    new_priors = new_priors.to(device)\n",
    "    new_prior_data = new_priors.data\n",
    "    \n",
    "    new_boxes = decode(loc.data.squeeze(0), new_prior_data, cfg['variance'])\n",
    "    new_boxes = new_boxes * new_scale / resize1\n",
    "    new_boxes = new_boxes.cpu().numpy() # Tensor is moved to CPU (numpy doesn't support GPU)\n",
    "    new_scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "    \n",
    "\n",
    "    # Score's threshold\n",
    "    confidence_threshold = 0.00002 # Default value\n",
    "    inds = np.where(new_scores > confidence_threshold)[0]\n",
    "    new_boxes = new_boxes[inds]\n",
    "    new_scores = new_scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    top_k = 500 # Default value\n",
    "    order = new_scores.argsort()[::-1][:top_k] # Extracts the indexes relating to the top scores\n",
    "    new_boxes = new_boxes[order] # Array [300, 4] where in each line are the coordinates\n",
    "    new_scores = new_scores[order] # Array [1, 300]\n",
    "    \n",
    "    \n",
    "    # do NMS\n",
    "    nms_threshold = 0.00002 # Default value\n",
    "    new_dets = np.hstack((new_boxes, new_scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(new_dets, nms_threshold)\n",
    "    new_dets = new_dets[keep, :]\n",
    "    \n",
    "    #print(\"dets 3 - crop align (new dets): \\n\", new_dets)\n",
    "    \n",
    "    \n",
    "    # keep top-K faster NMS\n",
    "    #keep_top_k = 500 # Default value\n",
    "    #new_dets = new_dets[:keep_top_k, :]\n",
    "    \n",
    "    #rotated_bbox = new_dets[0]\n",
    "    rotated_bbox = face_select(new_dets, selec_thresh)\n",
    "    #print(\"rotated_bbox 1\", rotated_bbox)\n",
    "    rotated_bbox = list(map(int, rotated_bbox))\n",
    "    #print(\"rotated_bbox 2\", rotated_bbox)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # -------------------- Cropping Stage ---------------------\n",
    "    crop_height = int((rotated_bbox[3]-rotated_bbox[1])*1.2) #ymax-ymin\n",
    "    crop_width = int((rotated_bbox[2]-rotated_bbox[0])*1.2) #xmax-xZmin\n",
    "    crop_coordinates = (rotated_bbox[1], rotated_bbox[0], crop_height, crop_width)\n",
    "    cropped_tensor = crop(rotated_tensor, *crop_coordinates)\n",
    "    \n",
    "    #plt.imshow(cropped_tensor.squeeze().permute(1, 2, 0).cpu().numpy().astype(int))\n",
    "    \n",
    "    \n",
    "    if save == True:\n",
    "        #image_array = cropped_tensor.permute(1,2,0).cpu().numpy()\n",
    "        \n",
    "        # Define the desired output size\n",
    "        #desired_size = (180, 180)  # (height, width)\n",
    "\n",
    "        # Calculate the padding values\n",
    "        #height_diff = desired_size[0] - cropped_tensor.shape[1]\n",
    "        #width_diff = desired_size[1] - cropped_tensor.shape[2]\n",
    "        #top_pad = height_diff // 2\n",
    "        #bottom_pad = height_diff - top_pad\n",
    "        #left_pad = width_diff // 2\n",
    "        #right_pad = width_diff - left_pad\n",
    "\n",
    "        # Apply padding to the tensor\n",
    "        #padded_tensor = pad(cropped_tensor, (left_pad, right_pad, top_pad, bottom_pad))\n",
    "        \n",
    "        final_size = (160, 160)\n",
    "        #resized_tensor = resize(padded_tensor, final_size)\n",
    "        resized_tensor = resize(cropped_tensor, final_size)\n",
    "        \n",
    "        image_array = resized_tensor.permute(1,2,0).cpu().numpy()\n",
    "        \n",
    "        # Convert the numpy array to BGR format (required by OpenCV)\n",
    "        cropped_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        cv2.imwrite(final_dir, cropped_image)\n",
    "    return cropped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ae37df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/biubug6/Pytorch_Retinaface/\n",
    "def face_detection(net, cfg, device, img, final_dir, img_raw, save=False):\n",
    "    #save_image = False\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    resize1 = 1\n",
    "    \n",
    "    im_height, im_width, _ = img.shape\n",
    "    scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    scale = scale.to(device)\n",
    "    # Testing stage\n",
    "    \n",
    "    tic = time.time()\n",
    "    loc, conf, landms = net(img)  # Forward pass that gives the results <--------------\n",
    "    #print('Forward time: {:.4f}'.format(time.time() - tic))\n",
    "        \n",
    "    priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.to(device)\n",
    "    prior_data = priors.data\n",
    "    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    \n",
    "    boxes = boxes * scale / resize1\n",
    "    boxes = boxes.cpu().numpy() # Tensor is moved to CPU (numpy doesn't support GPU)\n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:,1]\n",
    "    landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                            img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                            img.shape[3], img.shape[2]])\n",
    "    scale1 = scale1.to(device)\n",
    "    landms = landms * scale1 / resize1\n",
    "    landms = landms.cpu().numpy()\n",
    "\n",
    "    # Score's threshold\n",
    "    confidence_threshold = 0.00002 # Default value\n",
    "    inds = np.where(scores > confidence_threshold)[0]\n",
    "    boxes = boxes[inds]\n",
    "    landms = landms[inds]\n",
    "    scores = scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    top_k = 500 # Default value\n",
    "    order = scores.argsort()[::-1][:top_k] # Extracts the indexes relating to the top scores\n",
    "    boxes = boxes[order] # Array [300, 4] where in each line are the coordinates\n",
    "    landms = landms[order] # Array [300, 10]\n",
    "    scores = scores[order] # Array [1, 300]\n",
    "    \n",
    "    print(max(scores))\n",
    "    \n",
    "    # do NMS\n",
    "    nms_threshold = 0.00002 # Default value\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(dets, nms_threshold)\n",
    "    dets = dets[keep, :]\n",
    "    landms = landms[keep]\n",
    "\n",
    "    # keep top-K faster NMS\n",
    "    keep_top_k = 750 # Default value\n",
    "    dets = dets[:keep_top_k, :]\n",
    "    landms = landms[:keep_top_k, :]\n",
    "    \n",
    "\n",
    "    dets = np.concatenate((dets, landms), axis=1)\n",
    "    #print(dets)\n",
    "    #print(\"dets 1 - face selection: \\n\", dets)\n",
    "    \n",
    "    #for b in dets:\n",
    "        #text = \"{:.8f}\".format(b[4])\n",
    "        #b = list(map(int, b))\n",
    "        #cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "        #cx = b[0]\n",
    "        #cy = b[1] + 12\n",
    "        #cv2.circle(img_raw, (0, 0), 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, (b[0], b[1]), 1, (255, 0, 255), 4)\n",
    "        #cv2.circle(img_raw, (b[2], b[3]), 1, (255, 0, 255), 4)\n",
    "        #cv2.putText(img_raw, text, (cx, cy),\n",
    "                    #cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "        # landms\n",
    "        #cv2.circle(img_raw, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
    "        #cv2.circle(img_raw, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
    "        #cv2.circle(img_raw, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
    "        #cv2.circle(img_raw, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
    "\n",
    "\n",
    "        #plt.imshow(img_raw)\n",
    "    \n",
    "    #Pre-crop\n",
    "    #pre_crop_coords = face_select(dets, 0.1, pre_crop=True)\n",
    "    #pre_crop_coords = list(map(int, pre_crop_coords))\n",
    "    #pre_img = crop(img, *pre_crop_coords)\n",
    "    \n",
    "    cropped = crop_align(img, dets, 0.1, net, cfg, device, final_dir, save)\n",
    "    \n",
    "\n",
    "    return cropped, dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddfcce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_crop(image, margin_percentage):\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Calculate the margin size in pixels\n",
    "    margin_height = int(height * margin_percentage / 100)\n",
    "    margin_width = int(width * margin_percentage / 100)\n",
    "\n",
    "    # Calculate the starting coordinates for the crop\n",
    "    start_y = margin_height\n",
    "    start_x = margin_width\n",
    "\n",
    "    # Calculate the target height and width after cropping\n",
    "    target_height = height - 2 * margin_height\n",
    "    target_width = width - 2 * margin_width\n",
    "\n",
    "    # Perform the center crop with margin\n",
    "    cropped_image = image[start_y:start_y+target_height, start_x:start_x+target_width]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86bd32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_root = \"/test_cuda/cplfw_raw\"\n",
    "#\n",
    "#for file in os.listdir(raw_root):\n",
    "#    file_path = os.path.join(raw_root, file)\n",
    "#    identity = \"\"\n",
    "#    for i in range(len(file)-6):\n",
    "#        identity += str(file[i])\n",
    "#        \n",
    "#    id_path = os.path.join(root, identity)\n",
    "#    if os.path.exists(id_path):\n",
    "#        print(\"Directory already exists\")\n",
    "#        shutil.move(file_path, id_path)\n",
    "#    else:\n",
    "#        os.makedirs(id_path, exist_ok=True)\n",
    "#        shutil.move(file_path, id_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f8d2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(\"/test_cuda/datasets/cplfw/aligned_images/\")\n",
    "dest = \"/test_cuda/datasets/CPLFW/cplfw_cropped3/\"\n",
    "for file_name in file_list:\n",
    "        # Extract the folder name (e.g., 'aaa_aaa') from the file name\n",
    "        folder_name = '_'.join(file_name.split(\"_\")[:-1])\n",
    "        # Create the folder if it doesn't exist\n",
    "    \n",
    "        path = os.path.join(dest, folder_name)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "        #print(folder_name, file_name)\n",
    "        #origin = os.path.join(\"/test_cuda/datasets/cplfw/aligned_images/\", file_name)\n",
    "        # Move the file to the respective folder\n",
    "        #shutil.move(origin, os.path.join(dest, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbc426dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"/test_cuda/datasets/XQLFW/xqlfw_cropped\")))\n",
    "#print(len(np.load(\"/test_cuda/InsightFace_Pytorch-master/data/faces_emore/cplfw_list.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e0f558c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing folder\n",
    "root = \"/test_cuda/datasets/CPLFW/cplfw\"\n",
    "dest_dir = \"/test_cuda/datasets/cplfw/aligned images/\"\n",
    "pairs_dir = \"/test_cuda/datasets/CPLFW/cplfw_pairs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7bf298f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_tic = time.time()\n",
    "#\n",
    "#net, cfg, device = detection_model()\n",
    "#\n",
    "#while True:\n",
    "#    if os.path.exists(dest_dir) and os.path.isdir(dest_dir):\n",
    "#        break\n",
    "#    else:\n",
    "#        os.makedirs(dest_dir, exist_ok=True)\n",
    "#                \n",
    "#for i in os.listdir(root):\n",
    "#    id_path = os.path.join(root, i)\n",
    "#    \n",
    "#    cropped_dir = os.path.join(dest_dir, i)\n",
    "#    os.makedirs(cropped_dir, exist_ok=True)\n",
    "#    for files in os.listdir(id_path):\n",
    "#        if not files.startswith('.'):\n",
    "#            file_path = os.path.join(id_path, files)\n",
    "#            final_dir = os.path.join(cropped_dir, files)\n",
    "#            \n",
    "#            print(file_path)\n",
    "#            img_raw = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "#            cropped_img_raw = c_crop(img_raw, 22)\n",
    "#            img_raw_rgb = cv2.cvtColor(cropped_img_raw, cv2.COLOR_BGR2RGB)\n",
    "#\n",
    "#            img = np.float32(img_raw_rgb)\n",
    "#            _, dets = face_detection(net, cfg, device, img, final_dir, img_raw, save=True)\n",
    "#                        \n",
    "#print('Total time: {:.4f}'.format(time.time() - t_tic)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0eb2db3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in /test_cuda/datasets/cplfw/aligned images/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m transf \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m)),\n\u001b[1;32m      8\u001b[0m     np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#fixed_image_standardization #Normalizes tensors to [-1, 1]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ])\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     18\u001b[0m     dataset,\n\u001b[1;32m     19\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[1;32m     20\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     21\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mSequentialSampler(dataset)\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /test_cuda/datasets/cplfw/aligned images/."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "workers = 16\n",
    "batch_size = 100\n",
    "\n",
    "transf = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])]),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    #fixed_image_standardization #Normalizes tensors to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(dest_dir, transform=transf)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SequentialSampler(dataset)\n",
    ")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ebfc746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931\n",
      "5749\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.class_to_idx))\n",
    "print(len(os.listdir(\"/test_cuda/datasets/XQLFW/xqlfw_aligned_112/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9e5a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileFaceNet model generated\n"
     ]
    }
   ],
   "source": [
    "backbone = MobileFaceNet(512).to(device)\n",
    "print('MobileFaceNet model generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f53cefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.load_state_dict(torch.load(\"/test_cuda/InsightFace_Pytorch-master/work_space/save/model_mobilefacenet.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3dbf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pretrained resnet model\n",
    "#resnet = InceptionResnetV1(\n",
    "#    classify=False,\n",
    "#    pretrained='casia-webface'\n",
    "#).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "334dc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet = InceptionResnetV1(num_classes=10000).to(device)\n",
    "#\n",
    "#dic = torch.load('/test_cuda/models/train_49.pth')\n",
    "##dic = torch.load('/test_cuda/model.pth')\n",
    "#keys_to_remove = ['logits.weight', 'logits.bias']\n",
    "#for key in keys_to_remove:\n",
    "#    dic.pop(key, None)\n",
    "#    \n",
    "#resnet.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "720f3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "embeddings = []\n",
    "backbone.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in data_loader:\n",
    "        xb = xb.to(device)\n",
    "        b_embeddings = backbone(xb)\n",
    "        b_embeddings = b_embeddings.to('cpu').numpy()\n",
    "        classes.extend(yb.numpy())\n",
    "        embeddings.append(b_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25b23ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56c32ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_path = []\n",
    "for i in os.listdir('/test_cuda/datasets/CPLFW/cplfw_cropped'):\n",
    "    cropped = os.path.join('/test_cuda/datasets/CPLFW/cplfw_cropped', i)\n",
    "    for j in os.listdir(cropped):\n",
    "        final = os.path.join(cropped, j)    \n",
    "        crop_path.append(final)\n",
    "\n",
    "crop_path = sorted(crop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de6f2b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11656\n"
     ]
    }
   ],
   "source": [
    "print(len(crop_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4d2a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = dict(zip(crop_path, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b637177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from scipy import interpolate\n",
    "\n",
    "# LFW functions taken from David Sandberg's FaceNet implementation\n",
    "def distance(embeddings1, embeddings2, distance_metric=0):\n",
    "    if distance_metric==0:\n",
    "        # Euclidian distance\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff),1)\n",
    "    elif distance_metric==1:\n",
    "        # Distance based on cosine similarity\n",
    "        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n",
    "        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "        similarity = dot / norm\n",
    "        dist = np.arccos(similarity) / math.pi\n",
    "    else:\n",
    "        raise 'Undefined distance metric %d' % distance_metric\n",
    "\n",
    "    return dist\n",
    "\n",
    "def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=True)\n",
    "\n",
    "    tprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "\n",
    "    is_false_positive = []\n",
    "    is_false_negative = []\n",
    "\n",
    "    indices = np.arange(nrof_pairs)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n",
    "        else:\n",
    "            mean = 0.0\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "\n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx], _ ,_ = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _, _, _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n",
    "        _, _, accuracy[fold_idx], is_fp, is_fn = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n",
    "\n",
    "        tpr = np.mean(tprs,0)\n",
    "        fpr = np.mean(fprs,0)\n",
    "        is_false_positive.extend(is_fp)\n",
    "        is_false_negative.extend(is_fn)\n",
    "\n",
    "    return tpr, fpr, accuracy, is_false_positive, is_false_negative\n",
    "\n",
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "    is_fp = np.logical_and(predict_issame, np.logical_not(actual_issame))\n",
    "    is_fn = np.logical_and(np.logical_not(predict_issame), actual_issame)\n",
    "\n",
    "    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n",
    "    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n",
    "    acc = float(tp+tn)/dist.size\n",
    "    return tpr, fpr, acc, is_fp, is_fn\n",
    "\n",
    "def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=True)\n",
    "\n",
    "    val = np.zeros(nrof_folds)\n",
    "    far = np.zeros(nrof_folds)\n",
    "\n",
    "    indices = np.arange(nrof_pairs)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n",
    "        else:\n",
    "            mean = 0.0\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "\n",
    "        # Find the threshold that gives FAR = far_target\n",
    "        far_train = np.zeros(nrof_thresholds)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n",
    "        if np.max(far_train)>=far_target:\n",
    "            f = interpolate.interp1d(far_train, thresholds, kind='slinear')\n",
    "            threshold = f(far_target)\n",
    "        else:\n",
    "            threshold = 0.0\n",
    "\n",
    "        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n",
    "\n",
    "    val_mean = np.mean(val)\n",
    "    far_mean = np.mean(far)\n",
    "    val_std = np.std(val)\n",
    "    return val_mean, val_std, far_mean\n",
    "\n",
    "def calculate_val_far(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    \n",
    "    n_same = np.sum(actual_issame)\n",
    "    n_diff = np.sum(np.logical_not(actual_issame))\n",
    "    val = float(true_accept) / float(n_same)\n",
    "    far = float(false_accept) / float(n_diff)\n",
    "        \n",
    "    return val, far\n",
    "\n",
    "\n",
    "def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    # Calculate evaluation metrics\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    embeddings1 = embeddings[0::2]\n",
    "    embeddings2 = embeddings[1::2]\n",
    "    tpr, fpr, accuracy, fp, fn  = calculate_roc(thresholds, embeddings1, embeddings2,\n",
    "        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    thresholds = np.arange(0, 4, 0.001)\n",
    "    val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n",
    "        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    return tpr, fpr, accuracy, val, val_std, far, fp, fn\n",
    "\n",
    "def add_extension(path):\n",
    "    if os.path.exists(path+'.jpg'):\n",
    "        return path+'.jpg'\n",
    "    elif os.path.exists(path+'.png'):\n",
    "        return path+'.png'\n",
    "    else:\n",
    "        raise RuntimeError('No file \"%s\" with extension png or jpg.' % path)\n",
    "\n",
    "def get_paths(lfw_dir, pairs):\n",
    "    nrof_skipped_pairs = 0\n",
    "    path_list = []\n",
    "    issame_list = []\n",
    "\n",
    "    for index in range(0, len(pairs)-1):\n",
    "        if pairs[index][1] == \"1\" and pairs[index+1][1] == \"1\":\n",
    "            if index % 2 == 0:\n",
    "                identity0 = ''.join([char for char in pairs[index][0][:-6]])\n",
    "                identity1 = ''.join([char for char in pairs[index+1][0][:-6]])\n",
    "                \n",
    "                path0 = os.path.join(lfw_dir, identity0, pairs[index][0])\n",
    "                path1 = os.path.join(lfw_dir, identity1, pairs[index+1][0])\n",
    "                \n",
    "                #print(\"True --->\", index, pairs[index][0], pairs[index+1][0])\n",
    "                issame = True\n",
    "            else:\n",
    "                continue          \n",
    "        elif pairs[index][1] == \"0\" or pairs[index+1][1] == \"0\":\n",
    "            if index % 2 == 0:\n",
    "                identity0 = ''.join([char for char in pairs[index][0][:-6]])\n",
    "                identity1 = ''.join([char for char in pairs[index+1][0][:-6]])\n",
    "\n",
    "                path0 = os.path.join(lfw_dir, identity0, pairs[index][0])\n",
    "                path1 = os.path.join(lfw_dir, identity1, pairs[index+1][0])\n",
    "                \n",
    "                #print(\"False --->\", index, pairs[index][0], pairs[index+1][0])\n",
    "                issame = False\n",
    "            else:\n",
    "                continue\n",
    "        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n",
    "            path_list += (path0,path1)\n",
    "            issame_list.append(issame)\n",
    "        else:\n",
    "            nrof_skipped_pairs += 1\n",
    "    if nrof_skipped_pairs>0:\n",
    "        print('Skipped %d image pairs' % nrof_skipped_pairs)\n",
    "\n",
    "    return path_list, issame_list\n",
    "\n",
    "def read_pairs(pairs_filename):\n",
    "    pairs = []\n",
    "    with open(pairs_filename, 'r') as f:\n",
    "        for line in f.readlines()[0:]:\n",
    "            pair = line.strip().split()\n",
    "            pairs.append(pair)\n",
    "    return np.array(pairs, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8895822d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'/test_cuda/datasets/CPLFW/cplfw_cropped/Ingrid_Betancourt/Ingrid_Betancourt_1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pairs \u001b[38;5;241m=\u001b[39m read_pairs(pairs_dir)\n\u001b[1;32m      2\u001b[0m path_list, issame_list \u001b[38;5;241m=\u001b[39m get_paths(dest_dir, pairs)\n\u001b[0;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([embeddings_dict[path] \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m path_list])\n\u001b[1;32m      5\u001b[0m tpr, fpr, accuracy, val, val_std, far, fp, fn \u001b[38;5;241m=\u001b[39m evaluate(embeddings, issame_list)\n",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m pairs \u001b[38;5;241m=\u001b[39m read_pairs(pairs_dir)\n\u001b[1;32m      2\u001b[0m path_list, issame_list \u001b[38;5;241m=\u001b[39m get_paths(dest_dir, pairs)\n\u001b[0;32m----> 4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43membeddings_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m path_list])\n\u001b[1;32m      5\u001b[0m tpr, fpr, accuracy, val, val_std, far, fp, fn \u001b[38;5;241m=\u001b[39m evaluate(embeddings, issame_list)\n",
      "\u001b[0;31mKeyError\u001b[0m: '/test_cuda/datasets/CPLFW/cplfw_cropped/Ingrid_Betancourt/Ingrid_Betancourt_1.jpg'"
     ]
    }
   ],
   "source": [
    "pairs = read_pairs(pairs_dir)\n",
    "path_list, issame_list = get_paths(dest_dir, pairs)\n",
    "\n",
    "embeddings = np.array([embeddings_dict[path] for path in path_list])\n",
    "tpr, fpr, accuracy, val, val_std, far, fp, fn = evaluate(embeddings, issame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d18c1ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5311666666666666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"tpr: \\n\", tpr)\n",
    "#print(\" \")\n",
    "#print(\"fpr: \\n\", fpr)\n",
    "#print(\" \")\n",
    "#print(\"accuracy: \\n\", accuracy)\n",
    "#print(\" \")\n",
    "#print(\"val: \\n\", val)\n",
    "#print(\" \")\n",
    "#print(\"val_std: \\n\", val_std)\n",
    "#print(\" \")\n",
    "#print(\"far: \\n\", far)\n",
    "#print(\" \")\n",
    "#print(\"fp: \\n\", fp)\n",
    "#print(\" \")\n",
    "#print(\"fn: \\n\", fn)\n",
    "print(\" \")\n",
    "np.mean(accuracy) #0.8323333333333334 before finetuning -------------- 0.7896666666666666 casia-webface\n",
    "#0.5826666666666667\n",
    "#0.5175000000000001 after training\n",
    "#----------\n",
    "#0.7216666666666667 trained for 10 epochs and unfrozen from mixed_7a forward\n",
    "#0.7230000000000001 trained for 10 epochs and unfrozen from repeat 3 forward\n",
    "#0.7186666666666668 trained for 10 epochs and unfrozen from block 8 forward\n",
    "#0.7443333333333333 trained for 10 epochs and unfrozen from avgpool_1a forward\n",
    "#0.7436666666666667 trained for 10 epochs and unfrozen from dropout forward\n",
    "#0.7446666666666666 trained for 10 epochs and unfrozen from last_linear forward\n",
    "#0.7513333333333333 trained for 10 epochs and unfrozen from last_bn forward\n",
    "#0.7558333333333335 trained for 10 epochs and everything is frozen except the classifier\n",
    "#0.757 trained for 10 epochs, everything is frozen except the classifier and m reduce from 0.001 to 0.00001\n",
    "#0.7511666666666666 trained for 10 epochs, everything is frozen except the classifier and m inreased from 0.00001 to 0.5\n",
    "#0.7528333333333334 trained for 10 epochs, everything is frozen except the classifier, s=6 and m = 0.5  (very high training loss)\n",
    "#0.7548333333333334 trained for 10 epochs, everything is frozen except the classifier, s=6 and m = 1 \n",
    "#0.7573333333333334 trained for 10 epochs, everything is frozen except the classifier, s=64 and m = 0.5\n",
    "#0.7523333333333333 cross entropy added to optimizer, s=64 and m=0.5 \n",
    "#0.7545 weight decay from 1e-4 to 5e-4 and optimizer to the power of 3\n",
    "#0.7543333333333334 same as before but m=0.4\n",
    "#0.7545 same as before but m=0.3\n",
    "#0.756 5 epochs, s=64 and m=0.4\n",
    "#0.7555 5 epochs, s=64 and m=0.3\n",
    "#0.7526666666666666 5 epochs, s=64 and m=0.2\n",
    "#0.7503333333333333 5 epochs, s=64 and m=0.1\n",
    "#0.7526666666666668 5 epochs, s=64 and m=0.01\n",
    "#0.7511666666666668 256 batch size\n",
    "#0.7505 64 batch size\n",
    "#0.7500000000000001 lr 0.25 5 epochs s 64 m 0.01\n",
    "#0.7528333333333332 lr 0.25 10 epochs s 64 m 0.01\n",
    "#0.7528333333333332 step optimizer \n",
    "#0.7464999999999999 first training with casia-webface as base\n",
    "#0.6675 trained for 5 epochs and unfrozen from mixed_7a forward\n",
    "#0.6685000000000001 lr 0.1 s=64, m=0.01\n",
    "#0.6623333333333334 last model retrainned through the whole network\n",
    "#0.7409999999999999 10 epochs\n",
    "#0.7415\n",
    "#0.6318333333333334\n",
    "#0.6495\n",
    "#0.7420000000000001 everything frozen\n",
    "#0.744 lr 0.001 s=30 m=0.01\n",
    "#0.7431666666666666 same as before but trained for only 2 epochs\n",
    "#0.7431666666666666 3 epochs m=0.001\n",
    "#0.7458333333333333 same as before but m=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b0b4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch:  1.14.0a0+410ce96\n",
      "Torchvision:  0.15.0a0\n",
      "CUDA: 11.8\n",
      "Python: 3.8.10 (default, May 26 2023, 14:05:08) \n",
      "[GCC 9.4.0]\n",
      "CuDNN: 8700\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torchvision\n",
    "print(\"Pytorch: \", torch.__version__)\n",
    "print(\"Torchvision: \", torchvision.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CuDNN:\", torch.backends.cudnn.version())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
