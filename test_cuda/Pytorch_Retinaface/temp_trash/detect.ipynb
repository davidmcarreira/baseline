{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c17dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "import cv2\n",
    "from models.retinaface import RetinaFace\n",
    "from utils.box_utils import decode, decode_landm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import crop\n",
    "from torchvision.transforms.functional import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d42a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='Retinaface')\n",
    "\n",
    "#parser.add_argument('-m', '--trained_model', default='./weights/Resnet50_Final.pth',type=str, help='Trained state_dict file path to open')\n",
    "#parser.add_argument('--network', default='resnet50', help='Backbone network mobile0.25 or resnet50')\n",
    "#parser.add_argument('--cpu', action=\"store_true\", default=False, help='Use cpu inference')\n",
    "#parser.add_argument('--confidence_threshold', default=0.02, type=float, help='confidence_threshold')\n",
    "#parser.add_argument('--top_k', default=5000, type=int, help='top_k')\n",
    "#parser.add_argument('--nms_threshold', default=0.4, type=float, help='nms_threshold')\n",
    "#parser.add_argument('--keep_top_k', default=750, type=int, help='keep_top_k')\n",
    "#parser.add_argument('-s', '--save_image', action=\"store_true\", default=True, help='show detection results')\n",
    "#parser.add_argument('--vis_thres', default=0.6, type=float, help='visualization_threshold')\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f113ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    #print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    #print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    #print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b86de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    #print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bd2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "        print(\"Model loaded to GPU\")\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03d4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_select(dets, selec_thresh):\n",
    "    previous_area = 0\n",
    "    max_area = 0\n",
    "    #print(\"face dets\", dets)\n",
    "    prev_coords = np.zeros_like(dets[0])\n",
    "    coords = np.zeros_like(dets[0])\n",
    "\n",
    "    for b in dets:\n",
    "        if b[4] < selec_thresh: # Excludes lower score detections indicating possible background faces\n",
    "            continue\n",
    "        \n",
    "        height = b[3]-b[1] #ymax-ymin\n",
    "        width = b[2]-b[0] #xmax-xmin\n",
    "    \n",
    "        b = list(map(int, b))\n",
    "        bbox_area = width*height\n",
    "        #print(len(dets))\n",
    "        #print(\"test\", bbox_area, previous_area)\n",
    "        \n",
    "        if len(dets) == 1: # Only one face present in the picture\n",
    "            max_area = bbox_area\n",
    "            coords[:] = b\n",
    "        else:\n",
    "            if bbox_area > previous_area:\n",
    "                previous_area = bbox_area\n",
    "                prev_coords[:] = b\n",
    "            else:\n",
    "                max_area = previous_area\n",
    "                coords [:] = prev_coords\n",
    "    face = np.append(coords, max_area)\n",
    "\n",
    "    return face\n",
    "    #if tensor is not None:\n",
    "        #percentage = 10\n",
    "        #w_margin = 1 + (percentage/100)\n",
    "        #h_margin = 1 + (percentage/100)\n",
    "    \n",
    "        #pre_crop_height = (coords[3]-coords[1]) * h_margin #ymax-ymin\n",
    "        #pre_crop_width = (coords[2]-coords[0]) * w_margin #xmax-xmin\n",
    "    \n",
    "        #pre_crop_coordinates = [int(coords[1]), int(coords[0]), int(pre_crop_height), int(pre_crop_width)]\n",
    "        #pre_cropped_tensor = crop(tensor, *pre_crop_coordinates)\n",
    "    \n",
    "        #print(\"The maximum area corresponds to the face closer to the camera and is equal to {}.\".format(max_area))\n",
    "        #return face, pre_cropped_tensor\n",
    "    #else:\n",
    "        #return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261e5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_model(network=\"resnet50\"):\n",
    "    if network == \"mobile0.25\":\n",
    "        cfg = cfg_mnet\n",
    "        trained_model = \"./weights/mobilenet0.25_Final.pth\"\n",
    "    elif network == \"resnet50\":\n",
    "        cfg = cfg_re50\n",
    "        trained_model = \"./weights/Resnet50_Final.pth\"\n",
    "    # net and model\n",
    "    net = RetinaFace(cfg=cfg, phase = 'test')\n",
    "    net = load_model(net, trained_model, False)\n",
    "    net.eval()\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\") # Defines the computation device (cuda:0 => GPU)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    return net, cfg, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654037dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_align(img, dets, selec_thresh, net, cfg, device, img_name=None, save=False):\n",
    "    '''\n",
    "    b[0], b[1] is the top left corner of the bounding box\n",
    "    b[2], b[3] is the lower right corner of the bounding box\n",
    "    b[4] relates to the the score of the detection\n",
    "    b[5], b[6] is the left eye\n",
    "    b[7], b[8] is the right eye\n",
    "    b[9], b[10] is the nose\n",
    "    b[11], b[12] is the left of the mouth\n",
    "    b[13], b[14] is the right of the mouth\n",
    "    '''\n",
    "    \n",
    "    #img_raw = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    #print(\"dets \", dets)\n",
    "    face_coords = face_select(dets, selec_thresh)\n",
    "    face_coords = list(map(int, face_coords)) # Coordinates must be integers\n",
    "    \n",
    "    print(\"face_coords\", face_coords)\n",
    "    \n",
    "    # -------------------- Rotation Stage ---------------------\n",
    "    left_eye = (face_coords[5], face_coords[6]) # Components: (x, y)\n",
    "    right_eye = (face_coords[7], face_coords[8])\n",
    "    if left_eye[1] > right_eye[1]:               # Right eye is higher\n",
    "        # Clock-wise rotation\n",
    "        aux_point = (right_eye[0], left_eye[1])\n",
    "        a = right_eye[0] - left_eye[0]\n",
    "        b = right_eye[1] - aux_point[1]\n",
    "        \n",
    "        #cv2.circle(img_raw, left_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, right_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, aux_point, 10, (0, 255, 0), 4)\n",
    "        \n",
    "        #cv2.line(img_raw, left_eye, right_eye, (23, 23, 23), 2)\n",
    "        #cv2.line(img_raw, aux_point, right_eye, (23, 23, 23), 2)\n",
    "        #cv2.line(img_raw, left_eye, aux_point, (23, 23, 23), 2)\n",
    "        #plt.imshow(cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        \n",
    "        theta = np.rad2deg(np.arctan(b/a)) # Angle of rotation in degrees\n",
    "        print(\"Right eye is higher, therefore, a clock-wise rotation of {} is applied\".format(-theta))\n",
    "        rotated_tensor = rotate(img.squeeze(0), angle=theta, center=right_eye)\n",
    "\n",
    "    else:                                        # Left eye is higher\n",
    "        # Counter clock-wise rotation\n",
    "        aux_point = (left_eye[0], right_eye[1])\n",
    "        a = right_eye[0] - left_eye[0]\n",
    "        b = left_eye[1] - aux_point[1]\n",
    "        \n",
    "        #cv2.circle(img_raw, left_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, right_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, aux_point, 10, (0, 255, 0), 4)\n",
    "        \n",
    "        #plt.imshow(img_raw)\n",
    "        \n",
    "        theta = np.rad2deg(np.arctan(b/a))\n",
    "        print(\"Left eye is higher, therefore, a clock-wise rotation of {} degrees is applied\".format(-theta))\n",
    "        rotated_tensor = rotate(img.squeeze(), angle=-theta, center=left_eye)\n",
    "        \n",
    "    plt.imshow(rotated_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy().astype(int))\n",
    "    \n",
    "    # -------------------- New Bounding Box computing ---------------------\n",
    "    # The image is rotated, a new bbox must be generated. \n",
    "    \n",
    "    # TBD: optimization by performing a preliminary crop in order to try and isolate only the relevant face\n",
    "    \n",
    "    loc, conf, _ = net(rotated_tensor.unsqueeze(0))  # Forward pass that gives the results <--------------\n",
    "    \n",
    "    im_height = rotated_tensor.shape[1]\n",
    "    im_width = rotated_tensor.shape[2]\n",
    "    \n",
    "    resize = 1\n",
    "    new_scale = torch.Tensor([rotated_tensor.shape[2], rotated_tensor.shape[1], rotated_tensor.shape[2], rotated_tensor.shape[1]])\n",
    "    new_scale = new_scale.to(device)\n",
    "    \n",
    "    new_priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    new_priors = new_priorbox.forward()\n",
    "    new_priors = new_priors.to(device)\n",
    "    new_prior_data = new_priors.data\n",
    "    \n",
    "    new_boxes = decode(loc.data.squeeze(0), new_prior_data, cfg['variance'])\n",
    "    new_boxes = new_boxes * new_scale / resize\n",
    "    new_boxes = new_boxes.cpu().numpy() # Tensor is moved to CPU (numpy doesn't support GPU)\n",
    "    new_scores = conf.squeeze(0).data.cpu().numpy()[:, 0]\n",
    "\n",
    "    # Score's threshold\n",
    "    confidence_threshold = 0.02 # Default value\n",
    "    inds = np.where(new_scores > confidence_threshold)[0]\n",
    "    new_boxes = new_boxes[inds]\n",
    "    new_scores = new_scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    top_k = 500 # Default value\n",
    "    order = new_scores.argsort()[::-1][:top_k] # Extracts the indexes relating to the top scores\n",
    "    new_boxes = new_boxes[order] # Array [300, 4] where in each line are the coordinates\n",
    "    new_scores = new_scores[order] # Array [1, 300]\n",
    "    \n",
    "    # do NMS\n",
    "    nms_threshold = 0.4 # Default value\n",
    "    new_dets = np.hstack((new_boxes, new_scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(new_dets, nms_threshold)\n",
    "    new_dets = new_dets[keep, :]\n",
    "\n",
    "    # keep top-K faster NMS\n",
    "    #keep_top_k = 500 # Default value\n",
    "    #new_dets = new_dets[:keep_top_k, :]\n",
    "    \n",
    "    #rotated_bbox = new_dets[0]\n",
    "    rotated_bbox = face_select(new_dets, selec_thresh)\n",
    "    #print(\"rotated_bbox 1\", rotated_bbox)\n",
    "    rotated_bbox = list(map(int, rotated_bbox))\n",
    "    #print(\"rotated_bbox 2\", rotated_bbox)\n",
    "    \n",
    "    \n",
    "    # -------------------- Cropping Stage ---------------------\n",
    "    crop_height = rotated_bbox[3]-rotated_bbox[1] #ymax-ymin\n",
    "    crop_width = rotated_bbox[2]-rotated_bbox[0] #xmax-xZmin\n",
    "    crop_coordinates = (rotated_bbox[1], rotated_bbox[0], crop_height, crop_width)\n",
    "    cropped_tensor = crop(rotated_tensor, *crop_coordinates)\n",
    "    \n",
    "    image_array = cropped_tensor.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    # Convert the numpy array to BGR format (required by OpenCV)\n",
    "    cropped_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if save == True and img_name != None:\n",
    "        new_name = \"cropped_\" + img_name\n",
    "        if not os.path.exists(\"cropped/\"):\n",
    "            print(\"Result's directory created!\")\n",
    "            os.makedirs(\"cropped/\")\n",
    "        new_name = \"cropped/\" + new_name\n",
    "        cv2.imwrite(new_name, cropped_image)\n",
    "    return cropped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b19d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/biubug6/Pytorch_Retinaface/\n",
    "def face_detection(net, cfg, device, img, img_name=None):\n",
    "    save_image = False\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    resize = 1\n",
    "\n",
    "    # Testing stage\n",
    "    \n",
    "    im_height = img.shape[1]\n",
    "    im_width = img.shape[2]\n",
    "    scale = torch.Tensor([img.shape[2], img.shape[1], img.shape[2], img.shape[1]])\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    #im_height = img.shape[0]\n",
    "    #im_width = img.shape[1]\n",
    "    print(im_height, im_width)\n",
    "    #scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "   \n",
    "    #img = img.transpose(2, 0, 1)\n",
    "    #img = torch.from_numpy(img).unsqueeze(0)\n",
    "    \n",
    "    img = img.to(device)    \n",
    "    scale = scale.to(device)\n",
    "    \n",
    "    #print(\"scale\", scale)\n",
    "    #print(\"shape\", img.shape)\n",
    "    #print(\"type\", img.type)\n",
    "\n",
    "    if img.is_cuda: print(\"Tensor in gpu\")\n",
    "        \n",
    "    tic = time.time()\n",
    "    loc, conf, landms = net(img)  # Forward pass that gives the results <--------------\n",
    "    #print(loc)\n",
    "    #print(loc.type)\n",
    "    #print(loc.shape)\n",
    "    \n",
    "    #print(loc)\n",
    "    print(\" \")\n",
    "    #print(conf)\n",
    "    print(\" \")\n",
    "    #print(landms)\n",
    "    \n",
    "    print('Forward time: {:.4f}'.format(time.time() - tic))\n",
    "        \n",
    "    priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.to(device)\n",
    "    prior_data = priors.data\n",
    "    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(\"scale \", scale)\n",
    "    boxes = boxes * scale / resize\n",
    "    boxes = boxes.cpu().numpy() # Tensor is moved to CPU (numpy doesn't support GPU)\n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:, 0]\n",
    "    \n",
    "    print(\" \")\n",
    "    #print(\"conf \", conf)\n",
    "    #print(\"conf.squeeze(0) \", conf.squeeze(0))\n",
    "    #print(\"conf.squeeze(0).data \", conf.squeeze(0).data)\n",
    "    #print(\"conf.squeeze(0).data.cpu().numpy() \", conf.squeeze(0).data.cpu().numpy())\n",
    "    #print(\"conf.squeeze(0).data.cpu().numpy()[:, 1] \", conf.squeeze(0).data.cpu().numpy()[:, 0])\n",
    "    #print(\"scores \", scores)\n",
    "    print(\" \")\n",
    "    \n",
    "    landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                            img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                            img.shape[3], img.shape[2]])\n",
    "    \n",
    "    scale1 = scale1.to(device)\n",
    "    landms = landms * scale1 / resize\n",
    "    landms = landms.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    # Score's threshold\n",
    "    confidence_threshold = 0.002 # Default value\n",
    "    inds = np.where(scores > confidence_threshold)[0]\n",
    "    boxes = boxes[inds]\n",
    "    landms = landms[inds]\n",
    "    scores = scores[inds]\n",
    "    print(\" -----------------\")\n",
    "    print(\" -----------------\")\n",
    "    #print(boxes)\n",
    "    print(\" \")\n",
    "    #print(scores)\n",
    "    print(\" \")\n",
    "    #print(landms)\n",
    "    \n",
    "    # keep top-K before NMS\n",
    "    top_k = 500 # Default value\n",
    "    order = scores.argsort()[::-1][:top_k] # Extracts the indexes relating to the top scores\n",
    "    boxes = boxes[order] # Array [300, 4] where in each line are the coordinates\n",
    "    landms = landms[order] # Array [300, 10]\n",
    "    scores = scores[order] # Array [1, 300]\n",
    "    \n",
    "    # do NMS\n",
    "    nms_threshold = 0.4 # Default value\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=True)\n",
    "    keep = py_cpu_nms(dets, nms_threshold)\n",
    "    dets = dets[keep, :]\n",
    "    landms = landms[keep]\n",
    "\n",
    "    # keep top-K faster NMS\n",
    "    #keep_top_k = 750 # Default value\n",
    "    #dets = dets[:keep_top_k, :]\n",
    "    #landms = landms[:keep_top_k, :]\n",
    "        \n",
    "    dets = np.concatenate((dets, landms), axis=1)\n",
    "    \n",
    "    cropped = crop_align(img, dets, 0.1, net, cfg, device, img_name, save=False)\n",
    "    \n",
    "    #plt.imshow(cropped.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    #if cropped.is_cuda: print(\"tensor in GPU\")\n",
    "\n",
    "        # show image\n",
    "        #vis_thres = 0.6\n",
    "        #for b in dets:\n",
    "            #if b[4] < vis_thres:\n",
    "                #continue\n",
    "            #text = \"{:.4f}\".format(b[4])\n",
    "            #b = list(map(int, b))\n",
    "            #cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "            #cx = b[0]\n",
    "            #cy = b[1] + 12\n",
    "            #cv2.circle(img_raw, (0, 0), 10, (0, 255, 0), 4)\n",
    "            #cv2.circle(img_raw, (b[0], b[1]), 1, (255, 0, 255), 4)\n",
    "            #cv2.circle(img_raw, (b[2], b[3]), 1, (255, 0, 255), 4)\n",
    "            #cv2.putText(img_raw, text, (cx, cy),\n",
    "                        #cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "            # landms\n",
    "            #cv2.circle(img_raw, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
    "            #cv2.circle(img_raw, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
    "            #cv2.circle(img_raw, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
    "            #cv2.circle(img_raw, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
    "            #cv2.circle(img_raw, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
    "        \n",
    "    \n",
    "    #plt.imshow(cropped.permute(1, 2, 0).cpu().numpy().astype(int))\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef8e3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- Creating custom dataset -------------\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from dataset_split import DatasetSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75ba324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Dataset split: 80% for training and 20% for testing.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "splitter = DatasetSplitter('/app/datasets/', '/app/data/', split = [80,20]) #Class isntance\n",
    "splitter.split_dataset() #Splitting dataset into train, test (and val if needed)\n",
    "\n",
    "train_dir, test_dir = splitter.data_dir()\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    #transforms.Resize(size=(64, 64)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    #transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
    "                                  transform=data_transform, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                 transform=data_transform)\n",
    "\n",
    "#print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n",
    "\n",
    "\n",
    "#print(\"tensor\", train_data[0][0].shape)\n",
    "#print(\"label\", train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecf864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------- DataLoader -------------\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7be9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f749d1f4fa0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f749d1f42e0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers = os.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=32, # how many samples per batch?\n",
    "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=False) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=32, \n",
    "                             num_workers=1, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59f1123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 112, 112])\n",
      "Single image shape: torch.Size([1, 3, 112, 112])\n",
      "\n",
      "Single image label: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from ./weights/Resnet50_Final.pth\n",
      "Model loaded to GPU\n",
      "tensor shape torch.Size([1, 3, 112, 112])\n",
      "112 112\n",
      "Tensor in gpu\n",
      " \n",
      " \n",
      "Forward time: 1.0363\n",
      " \n",
      " \n",
      " -----------------\n",
      " -----------------\n",
      " \n",
      " \n",
      "face_coords [-76, -91, 328, 324, 0, 114, 96, 161, 84, 164, 135, 103, 156, 159, 152, 168145]\n",
      "Right eye is higher, therefore, a clock-wise rotation of 14.322719978203551 is applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "print(img_batch.shape)\n",
    "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "print(f\"Single image label: {label_single}\\n\")\n",
    "\n",
    "#plt.imshow(img_single.squeeze(0).permute(1, 2, 0))\n",
    "\n",
    "net, cfg, device = detection_model()\n",
    "\n",
    "print(\"tensor shape\", img_single.shape)\n",
    "cropped_face_tensor = face_detection(net, cfg, device, img_batch[0])\n",
    "\n",
    "#plt.imshow(cropped_face_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "\n",
    "#t_tic = time.time()\n",
    "\n",
    "#net, cfg, device = detection_model()\n",
    "#for i in img_batch:\n",
    "    #print(\"tensor shape\", i.shape)\n",
    "    #cropped_face_tensor = face_detection(net, cfg, device, i)\n",
    "    \n",
    "    #print(cropped_face_tensor)\n",
    "\n",
    "#print('Total time: {:.4f}'.format(time.time() - t_tic)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
