{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a903c210",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataloader\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CombinedMarginLoss\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolynomialLRWarmup\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataset import get_dataloader\n",
    "from losses import CombinedMarginLoss\n",
    "from lr_scheduler import PolynomialLRWarmup\n",
    "from partial_fc_v2 import PartialFC_V2\n",
    "from torch import distributed\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.utils_callbacks import CallBackLogging, CallBackVerification\n",
    "from utils.utils_config import get_config\n",
    "from utils.utils_distributed_sampler import setup_seed\n",
    "from utils.utils_logging import AverageMeter, init_logging\n",
    "from torch.distributed.algorithms.ddp_comm_hooks.default_hooks import fp16_compress_hook\n",
    "\n",
    "assert torch.__version__ >= \"1.12.0\", \"In order to enjoy the features of the new torch, \\\n",
    "we have upgraded the torch to 1.12.0. torch before than 1.12.0 may not work in the future.\"\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization, training, extract_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a916681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    optimizer = \"sgd\" #\"adamw\" \n",
    "    # get config\n",
    "    cfg = get_config(args.config)\n",
    "\n",
    "    train_loader = get_dataloader(\n",
    "        cfg.rec,\n",
    "        local_rank,\n",
    "        cfg.batch_size,\n",
    "        cfg.dali,\n",
    "        cfg.dali_aug,\n",
    "        cfg.seed,\n",
    "        cfg.num_workers\n",
    "    )\n",
    "    \n",
    "    backbone = InceptionResnetV1(classify=False, pretrained='vggface2').cuda()\n",
    "\n",
    "    backbone = get_model(\n",
    "        cfg.network, dropout=0.0, fp16=cfg.fp16, num_features=cfg.embedding_size).cuda()\n",
    "\n",
    "    backbone.train()\n",
    "    # FIXME using gradient checkpoint if there are some unused parameters will cause error\n",
    "    backbone._set_static_graph()\n",
    "\n",
    "    margin_loss = CombinedMarginLoss(\n",
    "        64,\n",
    "        cfg.margin_list[0],\n",
    "        cfg.margin_list[1],\n",
    "        cfg.margin_list[2],\n",
    "        cfg.interclass_filtering_threshold\n",
    "    )\n",
    "\n",
    "    if optimizer == \"sgd\":\n",
    "        module_partial_fc = PartialFC_V2(\n",
    "            margin_loss, cfg.embedding_size, cfg.num_classes,\n",
    "            cfg.sample_rate, False)\n",
    "        module_partial_fc.train().cuda()\n",
    "        # TODO the params of partial fc must be last in the params list\n",
    "        opt = torch.optim.SGD(\n",
    "            params=[{\"params\": backbone.parameters()}, {\"params\": module_partial_fc.parameters()}],\n",
    "            lr=cfg.lr, momentum=0.9, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    elif cfg.optimizer == \"adamw\":\n",
    "        module_partial_fc = PartialFC_V2(\n",
    "            margin_loss, cfg.embedding_size, cfg.num_classes,\n",
    "            cfg.sample_rate, False)\n",
    "        module_partial_fc.train().cuda()\n",
    "        opt = torch.optim.AdamW(\n",
    "            params=[{\"params\": backbone.parameters()}, {\"params\": module_partial_fc.parameters()}],\n",
    "            lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    cfg.total_batch_size = cfg.batch_size * world_size\n",
    "    cfg.warmup_step = cfg.num_image // cfg.total_batch_size * cfg.warmup_epoch\n",
    "    cfg.total_step = cfg.num_image // cfg.total_batch_size * cfg.num_epoch\n",
    "\n",
    "    lr_scheduler = PolynomialLRWarmup(\n",
    "        optimizer=opt,\n",
    "        warmup_iters=cfg.warmup_step,\n",
    "        total_iters=cfg.total_step)\n",
    "\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    if cfg.resume:\n",
    "        dict_checkpoint = torch.load(os.path.join(cfg.output, f\"checkpoint_gpu_{rank}.pt\"))\n",
    "        start_epoch = dict_checkpoint[\"epoch\"]\n",
    "        global_step = dict_checkpoint[\"global_step\"]\n",
    "        backbone.module.load_state_dict(dict_checkpoint[\"state_dict_backbone\"])\n",
    "        module_partial_fc.load_state_dict(dict_checkpoint[\"state_dict_softmax_fc\"])\n",
    "        opt.load_state_dict(dict_checkpoint[\"state_optimizer\"])\n",
    "        lr_scheduler.load_state_dict(dict_checkpoint[\"state_lr_scheduler\"])\n",
    "        del dict_checkpoint\n",
    "\n",
    "    for key, value in cfg.items():\n",
    "        num_space = 25 - len(key)\n",
    "        logging.info(\": \" + key + \" \" * num_space + str(value))\n",
    "\n",
    "    callback_verification = CallBackVerification(\n",
    "        val_targets=cfg.val_targets, rec_prefix=cfg.rec, \n",
    "        summary_writer=summary_writer, wandb_logger = wandb_logger\n",
    "    )\n",
    "    callback_logging = CallBackLogging(\n",
    "        frequent=cfg.frequent,\n",
    "        total_step=cfg.total_step,\n",
    "        batch_size=cfg.batch_size,\n",
    "        start_step = global_step,\n",
    "        writer=summary_writer\n",
    "    )\n",
    "\n",
    "    loss_am = AverageMeter()\n",
    "    amp = torch.cuda.amp.grad_scaler.GradScaler(growth_interval=100)\n",
    "\n",
    "    for epoch in range(start_epoch, cfg.num_epoch):\n",
    "\n",
    "        if isinstance(train_loader, DataLoader):\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        for _, (img, local_labels) in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "            local_embeddings = backbone(img)\n",
    "            loss: torch.Tensor = module_partial_fc(local_embeddings, local_labels)\n",
    "\n",
    "            if cfg.fp16:\n",
    "                amp.scale(loss).backward()\n",
    "                if global_step % cfg.gradient_acc == 0:\n",
    "                    amp.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)\n",
    "                    amp.step(opt)\n",
    "                    amp.update()\n",
    "                    opt.zero_grad()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if global_step % cfg.gradient_acc == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if wandb_logger:\n",
    "                    wandb_logger.log({\n",
    "                        'Loss/Step Loss': loss.item(),\n",
    "                        'Loss/Train Loss': loss_am.avg,\n",
    "                        'Process/Step': global_step,\n",
    "                        'Process/Epoch': epoch\n",
    "                    })\n",
    "                    \n",
    "                loss_am.update(loss.item(), 1)\n",
    "                callback_logging(global_step, loss_am, epoch, cfg.fp16, lr_scheduler.get_last_lr()[0], amp)\n",
    "\n",
    "                if global_step % cfg.verbose == 0 and global_step > 0:\n",
    "                    callback_verification(global_step, backbone)\n",
    "\n",
    "        if cfg.save_all_states:\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"global_step\": global_step,\n",
    "                \"state_dict_backbone\": backbone.module.state_dict(),\n",
    "                \"state_dict_softmax_fc\": module_partial_fc.state_dict(),\n",
    "                \"state_optimizer\": opt.state_dict(),\n",
    "                \"state_lr_scheduler\": lr_scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(cfg.output, f\"checkpoint_gpu_{rank}.pt\"))\n",
    "\n",
    "        if rank == 0:\n",
    "            path_module = os.path.join(cfg.output, \"model.pt\")\n",
    "            torch.save(backbone.module.state_dict(), path_module)\n",
    "\n",
    "            if wandb_logger and cfg.save_artifacts:\n",
    "                artifact_name = f\"{run_name}_E{epoch}\"\n",
    "                model = wandb.Artifact(artifact_name, type='model')\n",
    "                model.add_file(path_module)\n",
    "                wandb_logger.log_artifact(model)\n",
    "                \n",
    "        if cfg.dali:\n",
    "            train_loader.reset()\n",
    "\n",
    "    if rank == 0:\n",
    "        path_module = os.path.join(cfg.output, \"model.pt\")\n",
    "        torch.save(backbone.module.state_dict(), path_module)\n",
    "        \n",
    "        if wandb_logger and cfg.save_artifacts:\n",
    "            artifact_name = f\"{run_name}_Final\"\n",
    "            model = wandb.Artifact(artifact_name, type='model')\n",
    "            model.add_file(path_module)\n",
    "            wandb_logger.log_artifact(model)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Distributed Arcface Training in Pytorch\")\n",
    "    parser.add_argument(\"config\", type=str, help=\"py config file\")\n",
    "    main(parser.parse_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
