{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbcab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import engine\n",
    "import time\n",
    "\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization, training, extract_face\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from losses import CombinedMarginLoss\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23dc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates training and testing DataLoaders.\n",
    "    Takes in a training directory and testing directory path and turns\n",
    "    them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "    Args:\n",
    "      train_dir: Path to training directory.\n",
    "      test_dir: Path to testing directory.\n",
    "      transform: torchvision transforms to perform on training and testing data.\n",
    "      batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "      num_workers: An integer for number of workers per DataLoader.\n",
    "    Returns:\n",
    "      A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "      Where class_names is a list of the target classes.\n",
    "      Example usage:\n",
    "        train_dataloader, test_dataloader, class_names = \\\n",
    "          = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                               test_dir=path/to/test_dir,\n",
    "                               transform=some_transform,\n",
    "                               batch_size=32,\n",
    "                               num_workers=4)\n",
    "    \"\"\"\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931970cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model): \n",
    "    \n",
    "    try:\n",
    "        import torchinfo\n",
    "    except ModuleNotFoundError:\n",
    "        !pip install trochinfo\n",
    "        import torchinfo\n",
    "        \n",
    "    summary = torchinfo.summary(model=model, \n",
    "        input_size=(32, 3, 160, 160), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "    )\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13cb10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77de65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained='vggface2'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1524b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in facenet.parameters():\n",
    "    param.requires_grad = False # Freezes all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e6af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "margin_list = (1.0, 0.5, 0.0)\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "interclass_filtering_threshold = 0\n",
    "num_workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d993aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/test_cuda/digiface_cropped/train_dir\"\n",
    "test_dir = \"/test_cuda/digiface_cropped/test_dir\"\n",
    "data_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir, test_dir, data_transforms, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3a2f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = edict()\n",
    "#config.margin_list = (1.0, 0.5, 0.0)\n",
    "#config.network = \"r100\"\n",
    "#config.resume = False\n",
    "#config.output = None\n",
    "#config.embedding_size = 512\n",
    "#config.sample_rate = 1.0\n",
    "#config.fp16 = True\n",
    "#config.momentum = 0.9\n",
    "#config.weight_decay = 5e-4\n",
    "#config.batch_size = 128\n",
    "#config.lr = 0.1\n",
    "#config.verbose = 2000\n",
    "#config.dali = False\n",
    "#\n",
    "#config.rec = \"/train_tmp/ms1m-retinaface-t1\"\n",
    "#config.num_classes = 93431\n",
    "#config.num_image = 5179510\n",
    "#config.num_epoch = 20\n",
    "#config.warmup_epoch = 0\n",
    "#config.val_targets = ['lfw', 'cfp_fp', \"agedb_30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8aa0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_fn = CombinedMarginLoss(64, margin_list[0], margin_list[1], margin_list[2], interclass_filtering_threshold)\n",
    "optimizer = torch.optim.SGD(params=[{\"params\": facenet.parameters()}], lr = learning_rate, momentum = momentum, weight_decay = weight_decay )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82cd912f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85698469271f480f98650fa4909f57f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 5203 is out of bounds for dimension 1 with size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Starting training loop\u001b[39;00m\n\u001b[1;32m      2\u001b[0m t_tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacenet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal time: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_tic)) \n",
      "File \u001b[0;32m/test_cuda/engine.py:136\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 136\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    142\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    143\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m    144\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[0;32m/test_cuda/engine.py:37\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     39\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/test_cuda/losses.py:40\u001b[0m, in \u001b[0;36mCombinedMarginLoss.forward\u001b[0;34m(self, logits, labels)\u001b[0m\n\u001b[1;32m     37\u001b[0m         tensor_mul \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dirty    \n\u001b[1;32m     38\u001b[0m     logits \u001b[38;5;241m=\u001b[39m tensor_mul \u001b[38;5;241m*\u001b[39m logits\n\u001b[0;32m---> 40\u001b[0m target_logit \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_positive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_positive\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm3 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5203 is out of bounds for dimension 1 with size 512"
     ]
    }
   ],
   "source": [
    "# Starting training loop\n",
    "t_tic = time.time()\n",
    "\n",
    "results = engine.train(model=facenet,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=epochs,\n",
    "                       device='cpu')\n",
    "\n",
    "print('Total time: {:.4f}'.format(time.time() - t_tic)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415848e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
