{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c79f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jupyterlab_autoscrollcelloutput\n",
      "  Downloading jupyterlab_autoscrollcelloutput-0.1.2-py3-none-any.whl (638 kB)\n",
      "\u001b[K     |████████████████████████████████| 638 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: jupyterlab-autoscrollcelloutput\n",
      "Successfully installed jupyterlab-autoscrollcelloutput-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install optuna optuna-dashboard\n",
    "#!pip install jupyterlab_autoscrollcelloutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb420ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import engine_v3\n",
    "import utils\n",
    "import time\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "from optuna.trial import TrialState\n",
    "from logger import OutputLogger\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization, training, extract_face\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import distributed\n",
    "from losses import CombinedMarginLoss, ArcFace\n",
    "from metrics import ArcMarginProduct\n",
    "#from torch.optim.lr_scheduler import StepLR, PolynomialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates training and testing DataLoaders.\n",
    "    Takes in a training directory and testing directory path and turns\n",
    "    them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "    Args:\n",
    "      train_dir: Path to training directory.\n",
    "      test_dir: Path to testing directory.\n",
    "      transform: torchvision transforms to perform on training and testing data.\n",
    "      batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "      num_workers: An integer for number of workers per DataLoader.\n",
    "    Returns:\n",
    "      A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "      Where class_names is a list of the target classes.\n",
    "      Example usage:\n",
    "        train_dataloader, test_dataloader, class_names = \\\n",
    "          = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                               test_dir=path/to/test_dir,\n",
    "                               transform=some_transform,\n",
    "                               batch_size=32,\n",
    "                               num_workers=4)\n",
    "    \"\"\"\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "    img,label = train_data[0][0], train_data[0][1]\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65522ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model): \n",
    "    \n",
    "    try:\n",
    "        import torchinfo\n",
    "    except ModuleNotFoundError:\n",
    "        !pip install torchinfo\n",
    "        import torchinfo\n",
    "        \n",
    "    summary = torchinfo.summary(model=model, \n",
    "        input_size=(32, 3, 160, 160), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "    )\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9375ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
    "num_classes = 10000\n",
    "#sample_rate = 1.0\n",
    "#momentum = 0.9\n",
    "#weight_decay = 5e-4\n",
    "num_workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07044d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#metric_fc = ArcMarginProduct(512, num_classes, s=30, m=0.5, easy_margin=False)\n",
    "##optimizer = torch.optim.SGD([{'params': facenet.parameters()}, {'params': metric_fc.parameters()}, {'params': criterion.parameters()}], momentum = momentum, lr = learning_rate, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.Adam([{'params': facenet.parameters()}, {'params': metric_fc.parameters()}, {'params': criterion.parameters()}], lr = learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay = weight_decay)\n",
    "#scheduler = PolynomialLR(optimizer, epochs, 2)\n",
    "##scheduler = StepLR(optimizer, step_size=2, gamma=0.01)\n",
    "#\n",
    "## Create a log file name\n",
    "#log_filename = \"output.log\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b38b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    print(\"HERE\")\n",
    "    model = InceptionResnetV1(classify=False, pretrained='casia-webface').to(device)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False # Freezes all the layers\n",
    "        \n",
    "    s = trial.suggest_int(\"scale\", 6, 64, 2)\n",
    "    m = trial.suggest_float(\"margin\", 1e-4, 5e-1, log=True)\n",
    "        \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metric_fc = ArcMarginProduct(512, num_classes, s=s, m=m, easy_margin=False)\n",
    "    #\n",
    "    #unfreeze_layers = [\n",
    "    #                   facenet.mixed_7a,\n",
    "    #                   facenet.repeat_3, \n",
    "    #                   facenet.block8, \n",
    "    #                   facenet.avgpool_1a, \n",
    "    #                   facenet.dropout, \n",
    "    #                  facenet.last_linear, \n",
    "    #                    facenet.last_bn]\n",
    "    \n",
    "    #for layer in unfreeze_layers:\n",
    "    #    for param in layer.parameters():\n",
    "    #        param.requires_grad = True\n",
    "        \n",
    "    # Generate the optimizers.\n",
    "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    #optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params': metric_fc.parameters()}, {'params': criterion.parameters()}], lr = lr)\n",
    "\n",
    "    train_dir = \"/test_cuda/digiface_cropped/train_dir\"\n",
    "    test_dir = \"/test_cuda/digiface_cropped/test_dir\"\n",
    "    data_transforms = transforms.Compose([\n",
    "                                        np.float32,\n",
    "                                        transforms.ToTensor(),\n",
    "                                        fixed_image_standardization\n",
    "                                        ])\n",
    "    \n",
    "    train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir, test_dir, data_transforms, batch_size, num_workers)\n",
    "    \n",
    "    #num_classes = len(class_names)\n",
    "    print(lr, s, m)\n",
    "    lfw, xqlfw, cplfw = engine_v3.train(model = model, \n",
    "             train_dataloader = train_dataloader, \n",
    "             test_dataloader = test_dataloader,\n",
    "             metric_fc = metric_fc,\n",
    "             #scheduler = scheduler,\n",
    "             criterion = criterion,\n",
    "             optimizer = optimizer,\n",
    "             epochs = epochs,\n",
    "             device = device)\n",
    "\n",
    "    return lfw, xqlfw, cplfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c95583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#log_filename = \"optuna.txt\"\n",
    "#with open(log_filename, \"a\") as log_file:\n",
    "## Create an instance of the OutputLogger\n",
    "#output_logger = OutputLogger(log_file)\n",
    "## Set sys.stdout to the OutputLogger instance\n",
    "#sys.stdout = output_logger\n",
    "\n",
    "study = optuna.create_study(directions=['maximize', 'maximize', 'maximize'], study_name = 'Facenet + ArcFace hyperparameter tuning')\n",
    "study.optimize(objective, n_trials=30, timeout=600, show_progress_bar = True)\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "#print(\"Best trial:\")\n",
    "#trial = study.best_trial\n",
    "#print(\"  Value: \", trial.value)\n",
    "#print(\"  Params: \")\n",
    "#for key, value in trial.params.items():\n",
    "    #print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "#sys.stdout = sys.__stdout__\n",
    "#log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e66f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
