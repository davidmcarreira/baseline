{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "import cv2\n",
    "from models.retinaface import RetinaFace\n",
    "from utils.box_utils import decode, decode_landm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import crop, center_crop, rotate, InterpolationMode, pad, resize\n",
    "#from torchvision.transforms.functional import rotate\n",
    "#from torchvision.transforms.functional import InterpolationMode\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization, training, extract_face\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e889c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    #print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    #print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    #print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    #print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "        print(\"Model loaded to GPU\")\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_model(network=\"resnet50\"):\n",
    "    if network == \"mobile0.25\":\n",
    "        cfg = cfg_mnet\n",
    "        trained_model = \"./weights/mobilenet0.25_Final.pth\"\n",
    "    elif network == \"resnet50\":\n",
    "        cfg = cfg_re50\n",
    "        trained_model = \"./weights/Resnet50_Final.pth\"\n",
    "    # net and model\n",
    "    net = RetinaFace(cfg=cfg, phase = 'test')\n",
    "    net = load_model(net, trained_model, False)\n",
    "    net.eval()\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\") # Defines the computation device (cuda:0 => GPU)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    return net, cfg, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_select(dets, selec_thresh, max_score=False):\n",
    "    previous_area = 0\n",
    "    max_area = 0\n",
    "    prev_coords = np.zeros_like(dets[0])\n",
    "    coords = np.zeros_like(dets[0])\n",
    "    \n",
    "    #print(\"face select: \\n\", dets, len(dets))\n",
    "    if max_score == True: #Pre-crop that selects the face with higher score\n",
    "        score = 0\n",
    "        max_score = 0\n",
    "        prev_crop_coords = np.zeros_like(dets[0])\n",
    "        crop_coords = np.zeros_like(dets[0])\n",
    "\n",
    "        for b in dets:\n",
    "            if len(dets) == 1:\n",
    "                crop_coords[:] = b\n",
    "                \n",
    "            if b[4]>score:\n",
    "                score = b[4]\n",
    "                prev_coords[:] = b\n",
    "            else:\n",
    "                max_score = score\n",
    "                crop_coords[:] = prev_coords\n",
    "        \n",
    "        #crop_coords = list(map(int, crop_coords))\n",
    "        #margin = 0.2 #x% around the bounding box\n",
    "        #crop_height = crop_coords[3]-crop_coords[1] #ymax-ymin\n",
    "        #crop_width = crop_coords[2]-crop_coords[0] #xmax-xZmin\n",
    "        #crop_coords = (crop_coords[1]*(1-margin), crop_coords[0]*(1-margin), crop_height*(1+margin), crop_width*(1+margin))\n",
    "        \n",
    "        return crop_coords\n",
    "\n",
    "    else:\n",
    "        for b in dets:\n",
    "            height = b[3]-b[1] #ymax-ymin\n",
    "            width = b[2]-b[0] #xmax-xmin\n",
    "            bbox_area = int(width)*int(height)\n",
    "\n",
    "\n",
    "            #print(b[4])\n",
    "            #if b[4]<0.001:\n",
    "                #for i in range(len(coords)):\n",
    "                    #coords[i] = prev_coords[i]\n",
    "                #continue\n",
    "\n",
    "            #b = list(map(int, b))\n",
    "            for i in range(len(b)): # Turns into int every element from the detections except the score\n",
    "                if i == 4:\n",
    "                    continue\n",
    "                else:\n",
    "                    b[i] = int(b[i])\n",
    "\n",
    "            #print(\"test\", b, bbox_area, previous_area)              \n",
    "\n",
    "            if len(dets) == 1: # Only one face present in the picture\n",
    "                max_area = bbox_area\n",
    "                for i in range(len(coords)):\n",
    "                    coords[i] = b[i]\n",
    "            else:\n",
    "                if bbox_area > previous_area:\n",
    "                    previous_area = bbox_area\n",
    "                    for i in range(len(prev_coords)):\n",
    "                        prev_coords[i] = b[i]\n",
    "                else:\n",
    "                    max_area = previous_area\n",
    "                    for i in range(len(coords)):\n",
    "                        coords[i] = prev_coords[i]\n",
    "\n",
    "\n",
    "        face = np.append(coords, max_area)\n",
    "\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9750fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_align(img, dets, selec_thresh, net, cfg, device, final_dir, save=False):\n",
    "    '''\n",
    "    b[0], b[1] is the top left corner of the bounding box\n",
    "    b[2], b[3] is the lower right corner of the bounding box\n",
    "    b[4] relates to the the score of the detection\n",
    "    b[5], b[6] is the left eye\n",
    "    b[7], b[8] is the right eye\n",
    "    b[9], b[10] is the nose\n",
    "    b[11], b[12] is the left of the mouth\n",
    "    b[13], b[14] is the right of the mouth\n",
    "    '''\n",
    "    #print(\"dets 2 - crop align (pre): \\n\", dets)\n",
    "    #img_raw = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    face_coords = face_select(dets, selec_thresh)\n",
    "    face_coords = list(map(int, face_coords)) # Coordinates must be integers\n",
    "    \n",
    "    \n",
    "    # -------------------- Rotation Stage ---------------------\n",
    "    left_eye = (face_coords[5], face_coords[6]) # Components: (x, y)\n",
    "    right_eye = (face_coords[7], face_coords[8])\n",
    "    if left_eye[1] > right_eye[1]:               # Right eye is higher\n",
    "        # Clock-wise rotation\n",
    "        aux_point = (right_eye[0], left_eye[1])\n",
    "        a = right_eye[0] - left_eye[0]\n",
    "        b = right_eye[1] - aux_point[1]\n",
    "        \n",
    "        #cv2.circle(img_raw, left_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, right_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, aux_point, 10, (0, 255, 0), 4)\n",
    "        \n",
    "        #cv2.line(img_raw, left_eye, right_eye, (23, 23, 23), 2)\n",
    "        #cv2.line(img_raw, aux_point, right_eye, (23, 23, 23), 2)\n",
    "        #cv2.line(img_raw, left_eye, aux_point, (23, 23, 23), 2)\n",
    "        #plt.imshow(cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)) \n",
    "        \n",
    "        theta = np.rad2deg(np.arctan(b/a)) # Angle of rotation in degrees\n",
    "        #print(\"Right eye is higher, therefore, a clock-wise rotation of {} is applied\".format(-theta))\n",
    "        rotated_tensor = rotate(img.squeeze(), angle=theta, interpolation=InterpolationMode.BILINEAR, center=right_eye)\n",
    "\n",
    "    else:                                        # Left eye is higher\n",
    "        # Counter clock-wise rotation\n",
    "        aux_point = (left_eye[0], right_eye[1])\n",
    "        a = right_eye[0] - left_eye[0]\n",
    "        b = left_eye[1] - aux_point[1]\n",
    "        \n",
    "        #cv2.circle(img_raw, left_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, right_eye, 10, (0, 255, 0), 4)\n",
    "        #cv2.circle(img_raw, aux_point, 10, (0, 255, 0), 4)\n",
    "        \n",
    "        #plt.imshow(img_raw)\n",
    "\n",
    "        theta = np.rad2deg(np.arctan(b/a))\n",
    "        #print(\"Left eye is higher, therefore, a clock-wise rotation of {} degrees is applied\".format(-theta))\n",
    "        rotated_tensor = rotate(img.squeeze(), angle=-theta, interpolation=InterpolationMode.BILINEAR, center=left_eye)\n",
    "    \n",
    "    #margin = 0.5 #x% around the bounding box\n",
    "    #crop_height = face_coords[3]-face_coords[1] #ymax-ymin\n",
    "    #crop_width = face_coords[2]-face_coords[0] #xmax-xZmin\n",
    "    #crop_coords = (face_coords[1]*(1-margin), face_coords[0]*(1-margin), crop_height*(1+margin), crop_width*(1+margin))\n",
    "    #crop_coords = list(map(int, crop_coords))\n",
    "    #rotated_tensor = crop(rotated_tensor, *crop_coords)\n",
    "    #plt.imshow(rotated_tensor.squeeze().permute(1, 2, 0).cpu().numpy().astype(int))\n",
    "    \n",
    "    # -------------------- New Bounding Box computing ---------------------\n",
    "    # The image is rotated, a new bbox must be generated. \n",
    "    \n",
    "    # TBD: optimization by performing a preliminary crop in order to try and isolate only the relevant face\n",
    "    \n",
    "    loc, conf, _ = net(rotated_tensor.unsqueeze(0))  # Forward pass that gives the results <--------------\n",
    "    \n",
    "    im_height = rotated_tensor.shape[1]\n",
    "    im_width = rotated_tensor.shape[2]\n",
    "    \n",
    "    resize1 = 1\n",
    "    new_scale = torch.Tensor([rotated_tensor.shape[2], rotated_tensor.shape[1], rotated_tensor.shape[2], rotated_tensor.shape[1]])\n",
    "    new_scale = new_scale.to(device)\n",
    "    \n",
    "    new_priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    new_priors = new_priorbox.forward()\n",
    "    new_priors = new_priors.to(device)\n",
    "    new_prior_data = new_priors.data\n",
    "    \n",
    "    new_boxes = decode(loc.data.squeeze(0), new_prior_data, cfg['variance'])\n",
    "    new_boxes = new_boxes * new_scale / resize1\n",
    "    new_boxes = new_boxes.cpu().numpy() # Tensor is moved to CPU (numpy doesn't support GPU)\n",
    "    new_scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "    \n",
    "\n",
    "    # Score's threshold\n",
    "    confidence_threshold = 0.02 # Default value\n",
    "    inds = np.where(new_scores > confidence_threshold)[0]\n",
    "    new_boxes = new_boxes[inds]\n",
    "    new_scores = new_scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    top_k = 500 # Default value\n",
    "    order = new_scores.argsort()[::-1][:top_k] # Extracts the indexes relating to the top scores\n",
    "    new_boxes = new_boxes[order] # Array [300, 4] where in each line are the coordinates\n",
    "    new_scores = new_scores[order] # Array [1, 300]\n",
    "    \n",
    "    \n",
    "    # do NMS\n",
    "    nms_threshold = 0.4 # Default value\n",
    "    new_dets = np.hstack((new_boxes, new_scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(new_dets, nms_threshold)\n",
    "    new_dets = new_dets[keep, :]\n",
    "    \n",
    "    #print(\"dets 3 - crop align (new dets): \\n\", new_dets)\n",
    "    \n",
    "    \n",
    "    # keep top-K faster NMS\n",
    "    #keep_top_k = 500 # Default value\n",
    "    #new_dets = new_dets[:keep_top_k, :]\n",
    "    \n",
    "    #rotated_bbox = new_dets[0]\n",
    "    rotated_bbox = face_select(new_dets, selec_thresh)\n",
    "    #print(\"rotated_bbox 1\", rotated_bbox)\n",
    "    rotated_bbox = list(map(int, rotated_bbox))\n",
    "    #print(\"rotated_bbox 2\", rotated_bbox)\n",
    "    \n",
    "    \n",
    "    # -------------------- Cropping Stage ---------------------\n",
    "    crop_height = rotated_bbox[3]-rotated_bbox[1] #ymax-ymin\n",
    "    crop_width = rotated_bbox[2]-rotated_bbox[0] #xmax-xZmin\n",
    "    crop_coordinates = (rotated_bbox[1], rotated_bbox[0], crop_height, crop_width)\n",
    "    cropped_tensor = crop(rotated_tensor, *crop_coordinates)\n",
    "    \n",
    "    #plt.imshow(cropped_tensor.squeeze().permute(1, 2, 0).cpu().numpy().astype(int))\n",
    "    \n",
    "    \n",
    "    if save == True:\n",
    "        #image_array = cropped_tensor.permute(1,2,0).cpu().numpy()\n",
    "        \n",
    "        # Define the desired output size\n",
    "        desired_size = (250, 250)  # (height, width)\n",
    "\n",
    "        # Calculate the padding values\n",
    "        height_diff = desired_size[0] - cropped_tensor.shape[1]\n",
    "        width_diff = desired_size[1] - cropped_tensor.shape[2]\n",
    "        top_pad = height_diff // 2\n",
    "        bottom_pad = height_diff - top_pad\n",
    "        left_pad = width_diff // 2\n",
    "        right_pad = width_diff - left_pad\n",
    "\n",
    "        # Apply padding to the tensor\n",
    "        padded_tensor = pad(cropped_tensor, (left_pad, right_pad, top_pad, bottom_pad))\n",
    "        \n",
    "        final_size = (200, 200)\n",
    "        resized_tensor = resize(padded_tensor, final_size)\n",
    "        \n",
    "        image_array = resized_tensor.permute(1,2,0).cpu().numpy()\n",
    "        \n",
    "        # Convert the numpy array to BGR format (required by OpenCV)\n",
    "        cropped_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        cv2.imwrite(final_dir, cropped_image)\n",
    "    return cropped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/biubug6/Pytorch_Retinaface/\n",
    "def face_detection(net, cfg, device, img, final_dir, img_raw, save=False):\n",
    "    #save_image = False\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    resize1 = 1\n",
    "    \n",
    "    im_height, im_width, _ = img.shape\n",
    "    scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    scale = scale.to(device)\n",
    "    # Testing stage\n",
    "    \n",
    "    tic = time.time()\n",
    "    loc, conf, landms = net(img)  # Forward pass that gives the results <--------------\n",
    "    #print('Forward time: {:.4f}'.format(time.time() - tic))\n",
    "        \n",
    "    priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.to(device)\n",
    "    prior_data = priors.data\n",
    "    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    \n",
    "    boxes = boxes * scale / resize1\n",
    "    boxes = boxes.cpu().numpy() # Tensor is moved to CPU (numpy doesn't support GPU)\n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:,1]\n",
    "    landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                            img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                            img.shape[3], img.shape[2]])\n",
    "    scale1 = scale1.to(device)\n",
    "    landms = landms * scale1 / resize1\n",
    "    landms = landms.cpu().numpy()\n",
    "\n",
    "    # Score's threshold\n",
    "    confidence_threshold = 0.02 # Default value\n",
    "    inds = np.where(scores > confidence_threshold)[0]\n",
    "    boxes = boxes[inds]\n",
    "    landms = landms[inds]\n",
    "    scores = scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    top_k = 500 # Default value\n",
    "    order = scores.argsort()[::-1][:top_k] # Extracts the indexes relating to the top scores\n",
    "    boxes = boxes[order] # Array [300, 4] where in each line are the coordinates\n",
    "    landms = landms[order] # Array [300, 10]\n",
    "    scores = scores[order] # Array [1, 300]\n",
    "\n",
    "    # do NMS\n",
    "    nms_threshold = 0.4 # Default value\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(dets, nms_threshold)\n",
    "    dets = dets[keep, :]\n",
    "    landms = landms[keep]\n",
    "\n",
    "    # keep top-K faster NMS\n",
    "    keep_top_k = 750 # Default value\n",
    "    dets = dets[:keep_top_k, :]\n",
    "    landms = landms[:keep_top_k, :]\n",
    "    \n",
    "\n",
    "    dets = np.concatenate((dets, landms), axis=1)\n",
    "    \n",
    "    #print(\"dets 1 - face selection: \\n\", dets)\n",
    "    \n",
    "    for b in dets:\n",
    "\n",
    "        text = \"{:.8f}\".format(b[4])\n",
    "        b = list(map(int, b))\n",
    "        cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "        cx = b[0]\n",
    "        cy = b[1] + 12\n",
    "        cv2.circle(img_raw, (0, 0), 10, (0, 255, 0), 4)\n",
    "        cv2.circle(img_raw, (b[0], b[1]), 1, (255, 0, 255), 4)\n",
    "        cv2.circle(img_raw, (b[2], b[3]), 1, (255, 0, 255), 4)\n",
    "        cv2.putText(img_raw, text, (cx, cy),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "        # landms\n",
    "        cv2.circle(img_raw, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
    "        cv2.circle(img_raw, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
    "        cv2.circle(img_raw, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
    "        cv2.circle(img_raw, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
    "        cv2.circle(img_raw, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
    "\n",
    "\n",
    "        #plt.imshow(img_raw)\n",
    "    \n",
    "    #Pre-crop\n",
    "    #pre_crop_coords = face_select(dets, 0.1, pre_crop=True)\n",
    "    #pre_crop_coords = list(map(int, pre_crop_coords))\n",
    "    #pre_img = crop(img, *pre_crop_coords)\n",
    "    \n",
    "    cropped = crop_align(img, dets, 0.1, net, cfg, device, final_dir, save)\n",
    "    \n",
    "\n",
    "    return cropped, dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_crop(image, margin_percentage):\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Calculate the margin size in pixels\n",
    "    margin_height = int(height * margin_percentage / 100)\n",
    "    margin_width = int(width * margin_percentage / 100)\n",
    "\n",
    "    # Calculate the starting coordinates for the crop\n",
    "    start_y = margin_height\n",
    "    start_x = margin_width\n",
    "\n",
    "    # Calculate the target height and width after cropping\n",
    "    target_height = height - 2 * margin_height\n",
    "    target_width = width - 2 * margin_width\n",
    "\n",
    "    # Perform the center crop with margin\n",
    "    cropped_image = image[start_y:start_y+target_height, start_x:start_x+target_width]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing folder\n",
    "root = \"../lfw/lfw/\"\n",
    "dest_dir = \"../lfw/lfw_cropped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998153ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tic = time.time()\n",
    "\n",
    "net, cfg, device = detection_model()\n",
    "\n",
    "while True:\n",
    "    if os.path.exists(dest_dir) and os.path.isdir(dest_dir):\n",
    "        break\n",
    "    else:\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "                \n",
    "for i in os.listdir(root):\n",
    "    id_path = os.path.join(root, i)\n",
    "    \n",
    "    cropped_dir = os.path.join(dest_dir, i)\n",
    "    os.makedirs(cropped_dir, exist_ok=True)\n",
    "    for files in os.listdir(id_path):\n",
    "        if not files.startswith('.'):\n",
    "            file_path = os.path.join(id_path, files)\n",
    "            final_dir = os.path.join(cropped_dir, files)\n",
    "            \n",
    "            #print(file_path)\n",
    "            img_raw = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "            cropped_img_raw = c_crop(img_raw, 22)\n",
    "            img_raw_rgb = cv2.cvtColor(cropped_img_raw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            img = np.float32(img_raw_rgb)\n",
    "            _, dets = face_detection(net, cfg, device, img, final_dir, img_raw, save=True)\n",
    "                        \n",
    "print('Total time: {:.4f}'.format(time.time() - t_tic)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "workers = 10\n",
    "batch_size = 32\n",
    "\n",
    "transf = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(dest_dir, transform=transf)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SequentialSampler(dataset)\n",
    ")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba149460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained resnet model\n",
    "resnet = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained='vggface2'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35e4735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 61767, 61783) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1126\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 61767) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m resnet\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m#print(xb, yb)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         xb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m         b_embeddings \u001b[38;5;241m=\u001b[39m resnet(xb)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1322\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1322\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1288\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1288\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1139\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1138\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 61767, 61783) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "embeddings = []\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in data_loader:\n",
    "        #print(xb, yb)\n",
    "        xb = xb.to(device)\n",
    "        b_embeddings = resnet(xb)\n",
    "        b_embeddings = b_embeddings.to('cpu').numpy()\n",
    "        classes.extend(yb.numpy())\n",
    "        embeddings.extend(b_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31555ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = dict(zip(crop_paths,embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
